---
title: "Class08 Breast Cancer Mini Project"
author: "Joshua Khalil (A17784122)"
format: pdf
toc: true
---
## Background 
In today's class we will apply the methods amnd techniques clustering and PCA to help make sense of a real world breast cancer FNA (fine needle aspiration) biopsy  data set. 

```{r}

fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)


```
Now we want to omit the first column `diagnosis` column - I don't want to use it for my machine learning models. We will use it later to compare our results to the expert diagnosis. 

```{r}
wisc.data <- wisc.df[,-1]

```
Now we will set up a new vector called diagnosis that contains the data from the diagnosis column of the original dataset.

```{r}
diagnosis <-diagnosis <- wisc.df$diagnosis


```

>Q1. How many observations are in this dataset?

```{r}
nrow(wisc.df)
```
There are 569 observations in this data set. 

>Q2. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
sum(wisc.df$diagnosis == "M")
```
There are 212 malignant diagnosis in this data set. 

>Q3. How many variables/features in the data are suffixed with _mean?

```{r}

sum(grepl("_mean$", colnames(wisc.data)))
```
There are 10 variables in this data set.


## Performing PCA 

The next step in your analysis is to perform principal component analysis (PCA) on `wisc.data`. 

```{r}
colMeans(wisc.data)

apply(wisc.data,2,sd)
```


Execute the PCA with the `prcomp()` function on the wisc.data, scaling if appropriate, and assign the output model to `wisc.pr`

```{r}
wisc.pr <- prcomp(wisc.data, scale. = TRUE)
summary(wisc.pr)

```

>Q4. From your results, what proportion of the original variance is captured by the first principal component (PC1)?

44.27 %

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs are required to describe at least 70% of the original variance in the data. 


>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs are required to describe at least 90% of the original variance in the data. 


## Interpreting PCA results

Now we will use some visualizations to better understand your PCA model. A common visualization for PCA results is the so-called `biplot`. 

```{r}
biplot(wisc.pr)
```

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why? 

This plot is not easy to read it is all very clustered and not readable. 

Let's generate a more standard scatter plot of each observation along principal components 1 and 2. 


```{r}
library(ggplot2)
ggplot(wisc.pr$x) +
  aes(PC1, PC2, col=diagnosis) +
  geom_point()
```
>Q8 Generate a similar plot for principal components 1 and 3. What do you notice about these plots.

```{r}
ggplot(wisc.pr$x) +
  aes(PC1, PC3, col=diagnosis) +
  geom_point()

```

## Variance Explained

Let's make a scree plot to show how much variance each PC capture. We typically look for an “elbow” — a point where adding more PCs gives diminishing returns.

```{r}
pr.var <- (wisc.pr$sdev^2)
head(pr.var)
```


Let's calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. Let's call this variable `pve`.

```{r}
pve <- wisc.pr$sdev^2 / sum(wisc.pr$sdev^2)

plot(c(1,pve), xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

Alternative scree plot of the same data can be made...
```{r}
barplot(pve, ylab = "Percent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
Let's make the graph more attractive to the eye 

```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```


## Communicating PCA results

Large PC1 loading value (positive or negative) for one of the 30 original measurements (i.e. features/columns we started with), for example, would suggest that this feature is an important driver of the variation we see in the score plot and thus helpful for distinguishing “M” from “B” samples. Let’s check which features matter most to PC1

```{r}
sort(abs(wisc.pr$rotation[, 1]), decreasing = TRUE)[1:10]
```


>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC. Are there any features with larger contributions than this one?

```{r}
wisc.pr$rotation["concave.points_mean", 1]
```




## Hierarchical clustering

One of the optional arguments to `hclust()` allows you to pick different ways (a.k.a. “methods”) to link clusters together, with single, complete, and average being the most common “linkage methods”. You can explore the effects of these different methods in this section

```{r}
data.scaled <- scale(wisc.data)
data.dist <-dist(data.scaled)
wisc.hclust <- hclust(data.dist,method="complete" )

plot(wisc.hclust)
abline(h=19.5, col="red", lty=2)

```

>Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

At a height of around 19.5 the model had 4 clusters.


```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters, diagnosis)
```

Here we picked four clusters and see that cluster 1 largely corresponds to malignant cells (with diagnosis values of “M”) whilst cluster 3 largely corresponds to benign cells (with diagnosis values of “B”).


>Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

WardD2 minimizes within-cluster variance to create compact, spherical, and similarly-sized clusters. Complete linkage merges clusters based on the maximum distance between members, tending to produce more balanced,, yet sometimes elongated, clusters, and is less sensitive to outliers
```{r}
wisc.hclust <- hclust(data.dist,method="ward.D2" )

plot(wisc.hclust)
abline(h=19.5, col="red", lty=2)
```

## Combining Methods 

Here we wukk take our PCA results and use those as inputs for clustering. In other words our `wisc.pr$x` scores that we plotted above and use a subset of these PCs that capture most variance as input `hclust()`



```{r}
pc.dist <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust<- hclust(pc.dist, method="ward.D2")
plot (wisc.pr.hclust)

```
I want to know how the clustering in `grps` with values 1 and 2 correspond to expert diagnosis 

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)

table(grps, diagnosis)
```
>Q13. How well does the newly created hclust model with two clusters separate out the two “M” and “B” diagnoses?

One cluster is mostly malignant (179 M vs 24 B) The other cluster is mostly benign (333 B vs 33 M). This shows strong separation, with relatively few misclassifications.

We have 24 false positives, and 33 false negatives 


```{r}
ggplot(wisc.pr$x) +
  aes(PC1, PC2) +
  geom_point(col=grps)

```

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
table(wisc.pr.hclust.clusters , diagnosis)
```

> Q14. How well do the hierarchical clustering models you created in the previous sections (i.e. without first doing PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.hclust.clusters and wisc.pr.hclust.clusters) with the vector containing the actual diagnoses.


```{r}
table(wisc.hclust.clusters, diagnosis)
```

No single cluster cleanly separates M from B. There is considerably more mixing of diagnoses than with PCA-based clustering

Sensitivity TP/(TP+FN)
```{r}
179/(179+33)
```

Specificity TN/(TN+FP)
```{r}
333/(333+24)
```



## Prediction

We will use the `predict()` function that will take our PCA model from before and new cancer cell data and project that data onto our PCA space.


```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```



```{r}
plot(wisc.pr$x[,1:2], col=grps)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")

```


>Q 16 Which of these new patients should we prioritize for follow up based on your results?

Patient 2 is the one we should priortize the follow up study based on the results. 
